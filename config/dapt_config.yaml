# DAPT Training Configuration for PaleoBERT-Cambrian v1.0
#
# This configuration is optimized for 11GB VRAM (RTX 2080 Ti).
# Adjust batch_size and gradient_accumulation_steps if you have different VRAM.
#
# Usage:
#   python scripts/train_dapt.py --config config/dapt_config.yaml

# ============================================================================
# Model Configuration
# ============================================================================
model_name_or_path: "microsoft/deberta-v3-base"
tokenizer_path: "artifacts/tokenizer_v1"
gradient_checkpointing: true

# ============================================================================
# Data Configuration
# ============================================================================
# Training data: JSONL files with normalized Cambrian corpus
train_files:
  - "data/corpus_norm/train_*.jsonl"

# Evaluation data: Held-out Cambrian corpus
eval_files:
  - "data/corpus_norm/eval_*.jsonl"

# Maximum sequence length (DeBERTa max: 512)
max_seq_length: 512

# Number of workers for data loading
preprocessing_num_workers: 8

# ============================================================================
# Training Configuration (11GB VRAM Optimized)
# ============================================================================
output_dir: "checkpoints/paleo-dapt-v1"
overwrite_output_dir: true

# Batch configuration
# Effective batch size = per_device_train_batch_size * gradient_accumulation_steps * num_gpus
# With these settings: 8 * 16 * 1 = 128
per_device_train_batch_size: 8
per_device_eval_batch_size: 16
gradient_accumulation_steps: 16

# Memory optimization
fp16: true  # Mixed precision training (reduces VRAM usage)

# Optimization
learning_rate: 2.0e-4
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.0e-6
max_grad_norm: 1.0

# Learning rate schedule
lr_scheduler_type: "linear"
warmup_steps: 10000  # Linear warmup for 10k steps

# Training length
num_train_epochs: 3
max_steps: -1  # -1 means use num_train_epochs; set positive value to override

# Logging
logging_steps: 100  # Log every 100 steps

# Evaluation
eval_steps: 2000  # Evaluate every 2000 steps

# Checkpointing
save_steps: 10000  # Save checkpoint every 10k steps
save_total_limit: 3  # Keep only top-3 checkpoints
load_best_model_at_end: true
metric_for_best_model: "eval_loss"

# ============================================================================
# MLM Configuration
# ============================================================================
mlm_probability: 0.15  # 15% masking rate (standard for BERT/DeBERTa)

# ============================================================================
# Validation Configuration
# ============================================================================
# Domain-specific validation
rare_token_eval: true
test_terms_file: "artifacts/vocab/all_terms.txt"

# Early stopping
early_stopping_patience: 5
early_stopping_threshold: 0.01

# ============================================================================
# Miscellaneous
# ============================================================================
seed: 42

# Resume from checkpoint (leave null for fresh training)
resume_from_checkpoint: null
